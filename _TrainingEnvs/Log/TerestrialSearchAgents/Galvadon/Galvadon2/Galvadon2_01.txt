
	Cum nu am reusit sa utilizez metoda GiveModel() pentru Galvadon , dar ideea unui agent care sa caute mancare pentru el (creeata de erbivori), si tot el sa hraneasca erbivorii
	mi-a placut intrucat sustine viziunea unui ecosistem.
	Asa ca am decis sa creez un astfel de model. 

	Galvadon2_01:
	
	Antrenament FAZA 1:

		In prima faza agentul este instantiat in apropriere de centrul spatiului de antrenare , cu orientare aleatorie (directia in care se uita)
		De asemenea targetul este ales aleatoriu ( 0 - propria mancare , 1 - mancarea erbivorilor , 2 - erbivorii ) 
		Si exista 3 "gramezi" ce contin astfel de obiecte asezate in diferite parti.
		Vrem ca agentul sa invete legatura dintre target (primit ca observatie) si tag-ul pe care il atinge/vede cu razele sale.
		Si eventual noi am schimba din script, doar target-ul agentului pentru a avea un agent care rezolva cele 2 sarcini (de hranire proprie / hranire a erbivorilor) 

		--(Functia de reward)-- 

		-> -0.01 pe secunda (sau mai putin pana la -0.01 in functie de distanta pana la un partener compatibil) pentru a incuraja actiunea
		-> La atingerea peretilor rewardul este *setat* la -1. si se incheie episodul (prin Done(); )

		-> Cand atinge propria mancare      : +1 reward si done <=> target==0 sau -0.2 + Done() altfel
		-> Cand atinge mancarea erbivorilor : +1 reward si done <=> target==1 sau -0.2 + Done() altfel
		-> Cand atinge erbivorii            : +1 reward si done <=> target==2 sau -0.2 + Done() altfel		

		--(Observatii)--  

		Componenta parinte = Spatiul de antrenare
		Pozitia trebuie sa fie raportata la componenta parinte pentru ca , dupa ce terminam spatiul de antrenare il multiplicam pentru a antrena mai multi
		 agenti simultan	

		-> Pozitia raportata la componenta parinte , un vector3 normalizat (3 valori)
		-> Directia in care agentul inainteaza , un vector3 normalizat (3 valori)
		-> Directia in care se afla cea mai apropriata bucata de mancare , un vector 3 normalizat (3 valori)
		-> Distanta pana la cea mai apropriata bucata de mancare , un float (1 valoare)
		-> Target-ul agentului , un int (1 valoare)

		In total 11 + Observatiile de tip Raycast. De asemenea observatiile sunt stackate cate 3 
		(agentul are acces la observatiile prezente + ultimele 2 seturi de observatii primite)

		Raycast: 1 set configurat astfel: 4(stanga)-1(fata)-4(dreapta) - raze la unghi de 45 (cu 10 grade mai ascutit decat la Galvadon initial) ; distanta 50 , hitbox - 0.6   

		--(Actiunile)-- 

		Actiunile iau valori discrete:

		-agentul decide daca inainteaza sau nu 
		-daca se roteste (la stanga/dreapta) sau nu.

		--(TrainingConfig)--
		
		Voi incerca prima data antrenarea cu aceeasi configuratie.		

		Galvadon2Learning:
    			summary_freq: 5000         
   			time_horizon: 128           
   			beta: 1.0e-2                
    			batch_size: 2048            
    			buffer_size: 10240          
    			hidden_units: 256
    			num_layers: 2               
    			max_steps: 2.0e6 

		Log FAZA 1: Agentul atinge rezultate bune inca de pe la 600k pasi; la mancarea proprie sau la erbivori se duce din prima; pentru a ajunge la mancarea erbivorilor are un obicei
		ciudat de a ocolii multimea erbivorilor. Vom intarii politica in cea de-a doua faza.

		Am oprit antrenamentul la aproximativ 1.25m pasi , voi trece la faza a 2-a 


	Antrenament FAZA 2:

		Antrenam in mai multe faze pentru ca nu vrem sa facem overfitting pe spatiul de antrenare. 
		De aceasta data agentul este instantiat in partea de jos a spatiului de antrenare tot cu target ales aleatoriu 
		iar in scena se afla doar cate 1 element din fiecare target posibil. 

		Am adaugat 2m la numarul de pasi maxim alocati antrenamentului.

		Totusi la aproximativ 1.8m depasi (peste doar 600k pasi) am oprit antrenamentul in faza a doua pentru ca agentul se descurca foarte bine.
		
	Antrenament FINAL:

		In mediul de antrenament final, avem 3 agenti Mulak care se imperecheaza (functia de multiplicare dezactivata , pentru a fi mereu doar 4 agenti) si 1 agent Galvadon2
		Acesta trebuie sa: 

		1) ia mancarea erbivorilor (+0.5f)
		2) sa hraneasca un erbivor (+0.5f)
		3) sa manance floarea creeata de erbivorul hranit (+0.5f)
		4) Done(); (se reseteaza , si agentul ramane cu rewardul cumulat ; a se tine cont ca este penalizat pentru fiecare pas -> sper ca va fi incurajat sa termine mai repede) 
		
		Agentul rezultat antrenamentului din faza a doua se descurca bine , insa are probleme uneori cu evitarea erbivorilor pentru a ajunge la celelalte tipuri de tinte.

		Voi rula un antrenament asa , si daca modelul final nu este optim voi pleca tot de la Galvadon2_Faza2 adaugand -reward (mic) la contactul cu erbivorii daca nu ei sunt tinta.
		
		Am decis totusi sa adaug -1f/Maxstep reward pe frame in care agentul se atinge de un erbivor . (la 1.94m pasi)



		
		
	