 
	Am observat (devreme) in antrenament pentru versiunea 01 ca exista sansa ca agentul sa ia mancarea si apoi sa se loveasca de boundary ce rezulta in apelarea metodei Done()
	Astfel agentul era resetat dar mancarea nu. Am adaugat apelarea metodei ResetFood() la resetarea agentului.

	FAZA 2:

	Acum setup-ul spatiului de antrenare este diferit. Contine doar 1 animal erbivor (tot static) care este instantiat aleatoriu in scena.
	SI hrana este si ea asezata aleatoriu la fiecare resetare. 
	Agentul se descurca OK dupa FAZA 1 insa are cateva probleme pe care vrem sa le corectam:
		-> Cand tinta este la distanta mai mare nu stie sa se foloseasca indeajuns de bine de observatiile numerice (obiectele apropriate din prima scena au invatat agentul ca razele
		au o importanta mai mare)
		-> Uneori chiar daca tinta este mancarea , agentul merge dupa erbivor . 
		
		Nu am sa adaug -reward pe coliziunea cu erbivorul daca nu cara mancare pentru acesta deoarece:
		1) Nu vreau sa mai modific spatiul de observatii si sa adaug un bool care ofera agentului aceasta informatie (daca cara sau nu mancare)
		2) Am observat in antrenarea verziunilor initiale ca nu este optim sa ai +reward si -reward pentru aceeasi sarcina , diferenta sa fie anumiti parametrii.

		In schimb am sa elimin SetReward() la apelarea Switch-ului , vreau ca agentul sa fie afectat de timpul pierdut din cauza comportamentului ineficient 

	Am adaugat in trainer_config 2 milioane de pasi si voi continua antrenamentul cu --load

	La 2.6 mil de pasi am facut o pauza in antrenare pentru a face o modificare:
	
	Am observat ca agentul trece uneori chiar si de 3 ori prin animalul erbivor pana sa ia hrana (am adaugat functie de Reinstantiere aleatoriu in scena pentru erbivor la contactul 
	cu agentul Galvadon) insa nu e chiar corect , acum dupa 2.6m de pasi am adaugat si conditia ca agentul sa care mancare , doar in acest caz erbivorul va fi reasezat in scena 

	Testat cu agenti Mulak in scena , comportamentul nu este intocmai optim. asa ca voi antrena o retea mai puternica.