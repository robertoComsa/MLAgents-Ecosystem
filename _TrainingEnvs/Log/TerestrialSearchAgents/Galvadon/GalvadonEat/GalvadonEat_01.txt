
	Galvadon este un agent ce va folosi 2 modele (Retele neuronale) intr-un singur agent pentru a rezolva doua taskuri separate.
	Important este ca vectorul de actiuni si de observatii sa fie identic pentru ambele retele.

	Primul creier este GalvadonEat , ce va rezolva o problema asemanatoare lui Helios dar cu obiecte statice.
	Trebuie sa caute si sa gaseasca obiectele din scena cu tag-ul "galvadonFood". 

	Cel de-al doilea creier este GalvadonFeed , ce va rezolva sarcina de a hrani erbivorii. Aceasta sarcina este putin mai complexa , si va folosi o retea cu mai multi neuroni.
	Trebuie sa caute si sa gaseasca obiectele din scena cu tag-ul "preyFood" si apoi sa caute si sa duca mancarea la agentii erbivori.

	Pentru simplitate utilizarile acestor modele vor fi secventiale , o data agentul va manca el , apoi va cauta sa hraneasca un agent erbivor. 
 	Schimbarea creierului va fi facuta de insusi agent (el nu stie ca schimba creierul ci este incurajat sa apeleze metoda de Switch doar in anumite conditii - dupa ce rezolva o sarcina)

	GalvadonEat_01:
	
	Antrenament imperechere:

		--(Functia de reward)-- 

		-> -0.01 pe secunda (sau mai putin pana la -0.01 in functie de distanta pana la un partener compatibil) pentru a incuraja actiunea
		-> La atingerea peretilor rewardul este *setat* la -1. si se incheie episodul (prin Done(); )
		-> -0.1 de fiecare data cand apeleaza SwitchBrain() fara a fi terminat sarcina ( vectorAction[2] = 1)
		-> +0.5 cand agentul atinge hrana. 
		-> +0.5 cand agentul apeleaza SwitchBrain() dupa ce a mancat (a terminat sarcina) , de asemenea setam rewardul la 1f si incheiem acest episod de antrenare prin Done(); 
		
		--(Observatii)--

		Componenta parinte = Spatiul de antrenare
		Pozitia trebuie sa fie raportata la componenta parinte pentru ca , dupa ce terminam spatiul de antrenare il multiplicam pentru a antrena mai multi
		 agenti simultan	

		-> Pozitia raportata la componenta parinte , un vector3 normalizat (3 valori)
		-> Directia in care agentul inainteaza , un vector3 normalizat (3 valori)
		-> Directia in care se afla cea mai apropriata bucata de mancare , un vector 3 normalizat (3 valori)
		-> Distanta pana la cea mai apropriata bucata de mancare , un float (1 valoare)
		-> Daca agentul poate efectua switch-ul de creier , un int (1 valoare)

		In total 11 + Observatiile de tip Raycast. De asemenea observatiile sunt stackate cate 3 
		(agentul are acces la observatiile prezente + ultimele 2 seturi de observatii primite)

		Raycast: 1 set configurat astfel: 4(stanga)-1(fata)-4(dreapta) - raze la unghi de 55 ; distanta 50 , hitbox - 0.6   

		--(Actiunile)--

		Actiunile iau valori discrete:

		-agentul decide daca inainteaza sau nu 
		-daca se roteste (la stanga/dreapta) sau nu.
		-daca schimba creierul sau nu. A SE NOTA: In antrenarea creierului pentru prima sarcina , este folosita de agent pentru a reseta scena de antrenare dupa ce a terminat.

		--(TrainingConfig)--

		GalvadonEatLearning:
    			summary_freq: 5000         
   			time_horizon: 128           
   			beta: 1.0e-2                
    			batch_size: 2048            
    			buffer_size: 10240          
    			hidden_units: 128           
    			num_layers: 2               
    			max_steps: 2.0e6 

		Deoarece task-ul este unul relativ usor iar setup-ul este foarte bine structurat voi incerca cu 2 straturi cu 128 de neuroni.

		Agentul a ajuns la un reward optim inca de la 800k pasi dar antrenamentul a derulat 2m de pasi (pana la final).
		




		
		
	