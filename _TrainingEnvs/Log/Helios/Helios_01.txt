
	Helios va fi antrenat in 2 pasi. Prima data cu erbivori statici, pentru a fi antrenat dupa cu erbivori dinamici.
	
	Helios_01:

	Static:

		--(Functia de reward)-- 

		-> -0.02 pe secunda pentru a incuraja actiunea
		-> +0.2 cand mananca un erbivor
		-> La atingerea peretilor rewardul este *setat* la -1. si se incheie episodul (prin Done(); )

		Nota: Folosim un contor pentru a numara cati erbivori au fost mancati, la 6 incheiem episodul (tot prin Done(); )
		      0.2 x 6 = 1.2 reward maxim (daca agentul ar putea sa prinda instant toti erbivorii si nu ar fi penalizat pe secunda)
		      In principiu , vrem sa oprim antrenamentul cand avem rewardul in jur de 1. (probabil mai mult , si variaza deoarece erbivorii sunt plasati aleatoriu in scena)

		
		--(Observatii)--:

		Componenta parinte = Spatiul de antrenare
		Pozitia trebuie sa fie raportata la componenta parinte pentru ca , dupa ce terminam spatiul de antrenare il multiplicam pentru a antrena mai multi
		 agenti simultan	

		-> Pozitia raportata la componenta parinte , un vector3 normalizat (3 valori)
		-> Rotatia raportata la componenta parinte , un Quaternion normalizat (4 valori)
		-> Directia in care agentul inainteaza , un vector3 normalizat (3 valori)

		In total 10 + Observatiile de tip Raycast. De asemenea observatiile sunt stackate cate 3 (deci 10 x 3 + Raycast x 3 observatii)
		(agentul are acces la observatiile prezente + ultimele 2 seturi de observatii primite)

		--(Actiunile)--

		Actiunile iau valori discrete ,  agentul decide daca inainteaza sau nu , si daca se roteste (la stanga/dreapta) sau nu.

		--(TrainingConfig)--

		HeliosLearning:
    			summary_freq: 5000
    			time_horizon: 128
    			batch_size: 128
   			buffer_size: 2048
    			hidden_units: 128
    			beta: 1.0e-2
    			max_steps: 2.0e6
	